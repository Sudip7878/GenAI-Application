import streamlit as st
import os
from dotenv import load_dotenv

# LangChain & Groq
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Embeddings & Vector DB
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# -----------------------------
# Load environment / Streamlit secrets
# -----------------------------
load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN") or st.secrets.get("hf", {}).get("token")
GROQ_API_KEY = os.getenv("GROQ_API_KEY") or st.secrets.get("groq", {}).get("api_key")

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="Sewa Chatbot (Nepali)", layout="wide")
st.title("ЁЯУД Sewa Chatbot (рдиреЗрдкрд╛рд▓реА)")
st.write("рдХреГрдкрдпрд╛ PDF рдЕрдкрд▓реЛрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН рд░ рд╕реЛрдзреНрди рд╕реБрд░реБ рдЧрд░реНрдиреБрд╣реЛрд╕реНред")

# -----------------------------
# PDF Upload
# -----------------------------
uploaded_file = st.file_uploader("ЁЯУд рдпрд╣рд╛рдБ PDF рдЕрдкрд▓реЛрдб рдЧрд░реНрдиреБрд╣реЛрд╕реН", type=["pdf"])

retriever = None
if uploaded_file:
    with st.spinner("ЁЯУС PDF рдкрдвреНрджреИрдЫреБ..."):
        # Save to a temporary file
        temp_path = f"./temp_{uploaded_file.name}"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # Load and split
        loader = PyPDFLoader(temp_path)
        docs = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)

        # Embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"}
        )

        # Create vectorstore (in-memory, no persist)
        vectorstore = Chroma.from_documents(splits, embedding=embeddings)
        retriever = vectorstore.as_retriever()

        st.success("тЬЕ Vectorstore рддрдпрд╛рд░ рднрдпреЛред рдЕрдм рддрдкрд╛рдИрдВ рдкреНрд░рд╢реНрди рд╕реЛрдзреНрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫред")

# -----------------------------
# RAG Q&A Loop
# -----------------------------
if retriever:
    if prompt := st.chat_input("ЁЯСЙ рдкреНрд░рд╢реНрди рд▓реЗрдЦреНрдиреБрд╣реЛрд╕реН..."):
        st.chat_message("user").write(prompt)

        # Initialize Groq LLM
        llm = ChatGroq(api_key=GROQ_API_KEY, model="llama-3.3-70b-versatile")

        # System prompt enforcing Nepali-only answers
        system_prompt = (
            "рддрдкрд╛рдИрдБ рдПрдЙрдЯрд╛ рд╕рд╣рд╛рдпрдХ рд╕рд╣рд╛рдпрдХ рд╣реБрдиреБрд╣реБрдиреНрдЫред "
            "рд╕рдзреИрдВ рдХреЗрд╡рд▓ рдиреЗрдкрд╛рд▓реА рднрд╛рд╖рд╛рдорд╛ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд▓рддрдорд╛ рд╣рд┐рдиреНрджреА рд╡рд╛ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдкреНрд░рдпреЛрдЧ рдирдЧрд░реНрдиреБрд╣реЛрд╕реНред "
            "рджрд┐рдЗрдПрдХреЛ рдкреНрд░рд╕рдЩреНрдЧ (context) рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдпрджрд┐ рдкреНрд░рд╕рдЩреНрдЧ рдЦрд╛рд▓реА рдЫ рд╡рд╛ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЫреИрди рднрдиреЗ 'рдорд▓рд╛рдИ рдерд╛рд╣рд╛ рдЫреИрди' рднрдиреНрдиреБрд╣реЛрд╕реНред "
            "рдЬрд╡рд╛рдл рдЫреЛрдЯрдХрд░реАрдорд╛ рджрд┐рдиреБрд╣реЛрд╕реН (рдЕрдзрд┐рдХрддрдо рео рд╡рд╛рдХреНрдпрд╕рдореНрдо)ред\n\n{context}"
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        # Build chain
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Prepend explicit Nepali instruction
        user_input = "рдиреЗрдкрд╛рд▓реАрдорд╛ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реН: " + prompt

        # Run RAG
        rag_response = rag_chain.invoke({"input": user_input})
        answer = rag_response["answer"]

        # Show answer
        st.chat_message("assistant").write(answer)
